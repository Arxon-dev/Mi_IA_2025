import { QuestionTypeConfig, DifficultyLevelConfig } from './questionGeneratorService';
import { getBloomInstructions, BloomLevel } from './bloomTaxonomyService';
import { PromptService } from './promptService';
import { PromptValidationService } from './promptValidationService';
import { PrismaService } from './prismaService';
import { PromptSelectorService } from './promptSelectorService';

// INICIO DE NUEVOS IMPORTS
import { difficultyPrompt } from '../config/prompts/difficultyPrompt';
import { distractorsPrompt } from '../config/prompts/distractorsPrompt';
import { documentationPrompt } from '../config/prompts/documentationPrompt';
import { expertPrompt } from '../config/prompts/expertPrompt';
import { formatPrompt } from '../config/prompts/formatPrompt';
import { qualityPrompt } from '../config/prompts/qualityPrompt';
// FIN DE NUEVOS IMPORTS

export type AIProvider = 'openai' | 'anthropic' | 'google' | 'deepseek' | 'xai' | 'alibaba' | 'mistral' | 'cohere';

export interface AIModelConfig {
  modelId: string;
  apiEndpoint: string;
  maxTokens: number;
  temperature: number;
}

export interface AIModel {
  id: string;
  name: string;
  description: string;
  provider: AIProvider;
  config: AIModelConfig;
}

export const availableModels: AIModel[] = [
  {
    id: 'gemini-2.5-pro-preview-03-25',
    name: 'Gemini 2.5 Pro',
    description: 'Modelo m√°s avanzado de Google',
    provider: 'google',
    config: {
      modelId: 'gemini-2.5-pro-preview-03-25',
      apiEndpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent',
      maxTokens: 30720,
      temperature: 0.3
    }
  },
  {
    id: 'gemini-2.5-flash-preview-04-17',
    name: 'Gemini 2.5 flash',
    description: 'Modelo de lenguaje avanzado de Google con capacidades multimodales',
    provider: 'google',
    config: {
      modelId: 'gemini-2.5-flash-preview-04-17',
      apiEndpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent',
      maxTokens: 11000,
      temperature: 0.3
    }
  },
  {
    id: 'gemini-2.0-flash',
    name: 'Gemini 2.0 Flash',
    description: 'Versi√≥n r√°pida y eficiente de Gemini 2.0',
    provider: 'google',
    config: {
      modelId: 'gemini-2.0-flash',
      apiEndpoint: 'https://generativelanguage.googleapis.com/v1/models/gemini-2.0-flash:generateContent',
      maxTokens: 30720,
      temperature: 0.3
    }
  },
  {
    id: 'gpt-4',
    name: 'GPT-4',
    description: 'Modelo m√°s avanzado de OpenAI',
    provider: 'openai',
    config: {
      modelId: 'gpt-4',
      apiEndpoint: 'https://api.openai.com/v1/chat/completions',
      maxTokens: 8192,
      temperature: 0.3
    }
  },
  {
    id: 'gemini-2.0-flash-thinking',
    name: 'Google Gemini Flash Thinking',
    description: 'Modelo avanzado de Google combinando velocidad y rendimiento',
    provider: 'google',
    config: {
      modelId: 'gemini-2.0-flash-thinking-exp-01-21',
      apiEndpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-thinking-exp-01-21:generateContent',
      maxTokens: 30720,
      temperature: 0.3
    }
  },
  {
    id: 'claude-3-opus',
    name: 'Claude 3 Opus',
    description: 'Modelo m√°s avanzado de Anthropic',
    provider: 'anthropic',
    config: {
      modelId: 'claude-3-opus-20240229',
      apiEndpoint: 'https://api.anthropic.com/v1/messages',
      maxTokens: 4096,
      temperature: 0.3
    }
  },
  {
    id: 'claude-3-sonnet',
    name: 'Claude 3 Sonnet',
    description: 'Modelo intermedio de Anthropic',
    provider: 'anthropic',
    config: {
      modelId: 'claude-3-sonnet-20240229',
      apiEndpoint: 'https://api.anthropic.com/v1/messages',
      maxTokens: 40000, // L√≠mite oficial: https://docs.anthropic.com/claude/docs/models-overview
      temperature: 0.3
    }
  },
  {
    id: 'claude-3-haiku',
    name: 'Claude 3 Haiku',
    description: 'Modelo r√°pido y econ√≥mico de Anthropic',
    provider: 'anthropic',
    config: {
      modelId: 'claude-3-haiku-20240307',
      apiEndpoint: 'https://api.anthropic.com/v1/messages',
      maxTokens: 50000, // L√≠mite oficial: https://docs.anthropic.com/claude/docs/models-overview
      temperature: 0.3
    }
  },
  {
    id: 'deepseek-chat',
    name: 'Deepseek Chat',
    description: 'Modelo conversacional de Deepseek',
    provider: 'deepseek',
    config: {
      modelId: 'deepseek-chat',
      apiEndpoint: 'https://api.deepseek.com/v1/chat/completions',
      maxTokens: 8192,
      temperature: 0.3
    }
  },
  {
    id: 'gpt-4-5-preview',
    name: 'GPT-4.5 Preview',
    description: '√öltima versi√≥n preview de GPT-4.5',
    provider: 'openai',
    config: {
      modelId: 'gpt-4.5-preview-2025-02-27',
      apiEndpoint: 'https://api.openai.com/v1/chat/completions',
      maxTokens: 128000,
      temperature: 0.3
    }
  },
  {
    id: 'gpt-4o',
    name: 'GPT-4 Optimized',
    description: 'Versi√≥n optimizada de GPT-4',
    provider: 'openai',
    config: {
      modelId: 'gpt-4o',
      apiEndpoint: 'https://api.openai.com/v1/chat/completions',
      maxTokens: 128000,
      temperature: 0.3
    }
  },
  {
    id: 'gpt-4-turbo',
    name: 'GPT-4 Turbo',
    description: 'Versi√≥n m√°s r√°pida de GPT-4',
    provider: 'openai',
    config: {
      modelId: 'gpt-4-turbo',
      apiEndpoint: 'https://api.openai.com/v1/chat/completions',
      maxTokens: 128000,
      temperature: 0.3
    }
  },
  {
    id: 'gpt-3-5-turbo',
    name: 'GPT-3.5 Turbo',
    description: 'Modelo equilibrado de OpenAI',
    provider: 'openai',
    config: {
      modelId: 'gpt-3.5-turbo',
      apiEndpoint: 'https://api.openai.com/v1/chat/completions',
      maxTokens: 16385,
      temperature: 0.3
    }
  },
  {
    id: 'gpt-4-1',
    name: 'GPT-4.1',
    description: '√öltima versi√≥n avanzada de OpenAI (GPT-4.1)',
    provider: 'openai',
    config: {
      modelId: 'gpt-4.1-2025-04-14',
      apiEndpoint: 'https://api.openai.com/v1/chat/completions',
      maxTokens: 32768, // L√≠mite oficial por solicitud para GPT-4.1
      temperature: 0.3
    }
  },
  // Modelo a√±adido seg√∫n https://platform.openai.com/docs/models/gpt-4.1-mini y anuncios oficiales
  {
    id: 'gpt-4-1-mini',
    name: 'GPT-4.1 Mini',
    description: 'Versi√≥n mini de GPT-4.1, m√°s eficiente y econ√≥mica (OpenAI)',
    provider: 'openai',
    config: {
      modelId: 'gpt-4.1-mini-2025-04-14', // Nombre exacto seg√∫n la documentaci√≥n oficial
      apiEndpoint: 'https://api.openai.com/v1/chat/completions',
      maxTokens: 128000, // Seg√∫n capacidades anunciadas
      temperature: 0.3
    }
  },
  // Modelos Grok 3 de xAI a√±adidos seg√∫n https://x.ai/api y https://glama.ai/models/grok-3
  {
    id: 'grok-3',
    name: 'Grok 3',
    description: 'Modelo avanzado de xAI (Grok 3)',
    provider: 'xai',
    config: {
      modelId: 'grok-3',
      apiEndpoint: 'https://api.x.ai/v1/chat/completions', // Endpoint oficial xAI
      maxTokens: 131072, // Seg√∫n documentaci√≥n
      temperature: 0.3
    }
  },
  {
    id: 'grok-3-mini',
    name: 'Grok 3 Mini',
    description: 'Versi√≥n mini de Grok 3, m√°s eficiente y econ√≥mica (xAI)',
    provider: 'xai',
    config: {
      modelId: 'grok-3-mini',
      apiEndpoint: 'https://api.x.ai/v1/chat/completions',
      maxTokens: 131072, // Seg√∫n documentaci√≥n
      temperature: 0.3
    }
  },
  {
    id: 'claude-3-7-sonnet',
    name: 'Claude 3.7 Sonnet',
    description: 'Modelo Anthropic Claude 3.7 Sonnet (API oficial)',
    provider: 'anthropic',
    config: {
      modelId: 'claude-3-7-sonnet-20250219',
      apiEndpoint: 'https://api.anthropic.com/v1/messages',
      maxTokens: 200000, // Seg√∫n documentaci√≥n oficial
      temperature: 0.3
    }
  },
  {
    id: 'claude-3-5-haiku',
    name: 'Claude 3.5 Haiku',
    description: 'Modelo Anthropic Claude 3.5 Haiku (API oficial)',
    provider: 'anthropic',
    config: {
      modelId: 'claude-3-5-haiku-20241022',
      apiEndpoint: 'https://api.anthropic.com/v1/messages',
      maxTokens: 200000, // Seg√∫n documentaci√≥n oficial
      temperature: 0.3
    }
  },
  {
    id: 'claude-3-5-sonnet-v2',
    name: 'Claude 3.5 Sonnet v2',
    description: 'Modelo Anthropic Claude 3.5 Sonnet v2 (API oficial)',
    provider: 'anthropic',
    config: {
      modelId: 'claude-3-5-sonnet-latest',
      apiEndpoint: 'https://api.anthropic.com/v1/messages',
      maxTokens: 8000, // Seg√∫n documentaci√≥n oficial
      temperature: 0.3
    }
  },
  {
    id: 'claude-3-5-sonnet',
    name: 'Claude 3.5 Sonnet',
    description: 'Modelo Anthropic Claude 3.5 Sonnet (API oficial)',
    provider: 'anthropic',
    config: {
      modelId: 'claude-3-5-sonnet-20240620',
      apiEndpoint: 'https://api.anthropic.com/v1/messages',
      maxTokens: 8000, // Seg√∫n documentaci√≥n oficial
      temperature: 0.3
    }
  },
  {
    id: 'claude-sonnet-4-20250514',
    name: 'Claude Sonnet 4 (2025-05-14)',
    description: 'Modelo Sonnet 4 de Anthropic, versi√≥n 2025-05-14. Alto rendimiento, razonamiento avanzado, visi√≥n, multiling√ºe.',
    provider: 'anthropic',
    config: {
      modelId: 'claude-sonnet-4-20250514',
      apiEndpoint: 'https://api.anthropic.com/v1/messages',
      maxTokens: 64000, // M√°ximo de salida por petici√≥n seg√∫n documentaci√≥n oficial
      temperature: 0.3
    }
  }
];

type ApiKeyType = `${AIModel['provider']}ApiKey`;

export interface AIConfig {
  id: string;
  provider: string;
  model: string;
  apiKey: string | null;
  temperature: number | null;
  maxTokens: number | null;
  systemPrompt: string | null;
  textProcessing?: TextProcessingConfig;
  format?: FormatConfig;
  feedback?: FeedbackConfig;
  distribution?: DistributionConfig;
  questionsPerChunk?: number;
  createdAt: Date;
  updatedAt: Date;
  telegramSchedulerEnabled?: boolean;
  telegramSchedulerFrequency?: string;
  telegramSchedulerQuantity?: number;
  telegramSchedulerLastRun?: Date | null;
  telegramChatId?: string | null;
}

export interface AIConfigInput {
  id?: string;
  provider: string;
  model: string;
  apiKey?: string | null;
  temperature?: number | null;
  maxTokens?: number | null;
  systemPrompt?: string | null;
}

// Tipo para las respuestas de las APIs
interface AIModelResponse {
  text: string;
  error?: string;
}

export type OptionLengthType = 'muy_corta' | 'media' | 'larga' | 'aleatoria' | 'telegram';

export interface GenerateQuestionsConfig {
  types: QuestionTypeConfig[];
  difficulties: DifficultyLevelConfig[];
  bloomLevels?: BloomLevel[];
  advancedFeatures?: {
    conceptualTrap?: boolean;
    precisionDistractors?: boolean;
    legalTextProcessing?: boolean;
  };
  optionLength?: OptionLengthType; // Nueva opci√≥n para la longitud de las opciones de respuesta
}

export interface TextProcessingConfig {
  tokenLimit: number;
  minLength: number;
  maxLength: number;
  language: string;
  chunkSize: number;
  processBySection: boolean;
  maintainArticleOrder: boolean;
}

export interface FormatConfig {
  includeMnemonicRules: boolean;
  includePracticalCases: boolean;
  includeCrossReferences: boolean;
}

export interface FeedbackConfig {
  detailLevel: 'basic' | 'detailed';
  includeNormativeReferences: boolean;
  includeTopicConnections: boolean;
}

export interface DistributionConfig {
  sectionDistribution: {
    sectionId: string;
    percentage: number;
  }[];
  theoreticalPracticalRatio: number; // 0-100, donde 100 es todo te√≥rico
  difficultyTypeDistribution: {
    difficulty: string;
    typeDistribution: {
      type: string;
      percentage: number;
    }[];
  }[];
}

interface GenerationOptions {
  progressive?: boolean;
  batchSize?: number;
  continueFromLast?: boolean;
}

interface GenerationConfig {
  types: QuestionTypeConfig[];
  difficulties: DifficultyLevelConfig[];
  numberOfQuestions?: number;
}

interface Document {
  id: string;
  title: string;
  content: string;
  date: Date;
  type: string;
}

// Importar los prompts por defecto
import { expertPrompt as defaultExpertPrompt } from '../config/prompts/expertPrompt';
import { formatPrompt as defaultFormatPrompt } from '../config/prompts/formatPrompt';
import { difficultyPrompt as defaultDifficultyPrompt } from '../config/prompts/difficultyPrompt';
import { distractorsPrompt as defaultDistractorsPrompt } from '../config/prompts/distractorsPrompt';
import { documentationPrompt as defaultDocumentationPrompt } from '../config/prompts/documentationPrompt';
import { qualityPrompt as defaultQualityPrompt } from '../config/prompts/qualityPrompt';
import { telegramPrompt } from '../config/prompts/telegramPrompt';

const DEFAULT_PROMPTS: Record<string, string> = {
  expertPrompt: defaultExpertPrompt,
  formatPrompt: defaultFormatPrompt,
  difficultyPrompt: defaultDifficultyPrompt,
  distractorsPrompt: defaultDistractorsPrompt,
  documentationPrompt: defaultDocumentationPrompt,
  qualityPrompt: defaultQualityPrompt
};

export class AIService {
  private static config: AIConfig | null = null;
  private static isInitialized = false;
  private static prismaService = new PrismaService();
  private static useConceptTrap: boolean = false;
  private static usePrecisionDistractors: boolean = false;
  private static currentBatchSize = 0;
  private static API_KEY = process.env.NEXT_PUBLIC_GPT_API_KEY;
  private static API_URL = 'https://api.openai.com/v1/chat/completions';
  private static MODEL = 'gpt-4';

  private static readonly availableModels: Record<AIProvider, AIModel[]> = {
    google: [
      {
        id: 'gemini-2.5-pro-latest',
        name: 'Gemini 2.5 Pro',
        description: 'Modelo m√°s avanzado de Google (2024)',
        provider: 'google',
        config: {
          modelId: 'gemini-2.5-pro-latest',
          apiEndpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-latest:generateContent',
          maxTokens: 30720,
          temperature: 0.4
        }
      },
      {
        id: 'gemini-2.5-flash-preview-04-17',
        name: 'Gemini 2.5 Flash',
        description: 'Modelo r√°pido y eficiente de Google (2024)',
        provider: 'google',
        config: {
          modelId: 'gemini-2.5-flash-preview-04-17',
          apiEndpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-04-17:generateContent',
          maxTokens: 30720,
          temperature: 0.4
        }
      },
      {
        id: 'gemini-2.0-flash',
        name: 'Gemini 2.0 Flash',
        description: 'Versi√≥n r√°pida y eficiente de Gemini 2.0',
        provider: 'google',
        config: {
          modelId: 'gemini-2.0-flash',
          apiEndpoint: 'https://generativelanguage.googleapis.com/v1/models/gemini-2.0-flash:generateContent',
          maxTokens: 30720,
          temperature: 0.3
        }
      },
      {
        id: 'gemini-2.0-flash-thinking',
        name: 'Google Gemini Flash Thinking',
        description: 'Modelo avanzado de Google combinando velocidad y rendimiento',
        provider: 'google',
        config: {
          modelId: 'gemini-2.0-flash-thinking-exp-01-21',
          apiEndpoint: 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-thinking-exp-01-21:generateContent',
          maxTokens: 30720,
          temperature: 0.3
        }
      }
    ],
    openai: [
      {
        id: 'gpt-4',
        name: 'GPT-4',
        description: 'Modelo m√°s avanzado de OpenAI',
        provider: 'openai',
        config: {
          modelId: 'gpt-4',
          apiEndpoint: 'https://api.openai.com/v1/chat/completions',
          maxTokens: 8192,
          temperature: 0.3
        }
      }
    ],
    anthropic: [
      {
        id: 'claude-3-opus',
        name: 'Claude 3 Opus',
        description: 'Modelo m√°s avanzado de Anthropic',
        provider: 'anthropic',
        config: {
          modelId: 'claude-3-opus-20240229',
          apiEndpoint: 'https://api.anthropic.com/v1/messages',
          maxTokens: 4096,
          temperature: 0.3
        }
      }
    ],
    deepseek: [],
    xai: [],
    alibaba: [],
    mistral: [],
    cohere: []
  };

  // Utilidad para obtener la URL absoluta en backend y relativa en frontend
  static getApiConfigUrl() {
    const isServer = typeof window === 'undefined';
    const baseUrl = isServer ? process.env.NEXT_PUBLIC_BASE_URL || 'http://localhost:3000' : '';
    return isServer ? `${baseUrl}/api/ai-config` : '/api/ai-config';
  }

  static async initialize(): Promise<void> {
    console.log('üîÑ Inicializando AIService...');
    
    try {
      // Cargar configuraci√≥n desde la base de datos
      const apiConfigUrl = this.getApiConfigUrl();
      console.log('üìç URL de configuraci√≥n:', apiConfigUrl);
      
      try {
        const response = await fetch(apiConfigUrl);
        console.log('üìä Estado de respuesta:', response.status, response.statusText);
        
        if (!response.ok) {
          const errorText = await response.text().catch(() => 'No se pudo leer el cuerpo de la respuesta');
          console.error('‚ùå Error en respuesta:', errorText);
          throw new Error(`Error al cargar la configuraci√≥n: ${response.status} ${response.statusText}`);
        }
        
        const dbConfig = await response.json();
        console.log('üì• Configuraci√≥n cargada de la base de datos:', dbConfig);
        
        // Si no hay configuraci√≥n en la BD, crear una por defecto
        if (!dbConfig) {
          console.log('‚ö†Ô∏è No se encontr√≥ configuraci√≥n en la BD, creando configuraci√≥n por defecto...');
          const defaultConfig = {
            provider: 'google' as AIProvider,
            model: 'gemini-2.0-flash-thinking',
            temperature: 0.3,
            maxTokens: 8072
          };
          
          // Guardar configuraci√≥n por defecto en la BD
          const saveResponse = await fetch(this.getApiConfigUrl(), {
            method: 'PUT',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify(defaultConfig)
          });
          
          if (!saveResponse.ok) {
            const saveErrorText = await saveResponse.text().catch(() => 'No se pudo leer el cuerpo de la respuesta');
            console.error('‚ùå Error al guardar configuraci√≥n por defecto:', saveErrorText);
            throw new Error('Error al guardar la configuraci√≥n por defecto');
          }
          
          this.config = await saveResponse.json();
          console.log('‚úÖ Configuraci√≥n por defecto guardada:', this.config);
        } else {
          // Usar la configuraci√≥n existente sin modificarla
          this.config = dbConfig;
          console.log('üìù Usando configuraci√≥n existente:', {
            provider: this.config?.provider,
            model: this.config?.model
          });
        }
      } catch (fetchError) {
        console.error('‚ùå Error al realizar la petici√≥n fetch:', fetchError);
        throw fetchError;
      }
      
      // Verificar que tenemos una configuraci√≥n v√°lida
      if (!this.config?.provider || !this.config?.model) {
        console.error('‚ùå Configuraci√≥n inv√°lida:', this.config);
        throw new Error('Configuraci√≥n inv√°lida: falta provider o model');
      }

      // Verificar el estado de la API key
      if (this.config.apiKey) {
        await this.verificarEstadoAPIKey();
        console.log('‚úÖ API key verificada correctamente');
      } else {
        console.warn('‚ö†Ô∏è No hay API key configurada');
      }
      
      this.isInitialized = true;
      console.log('‚úÖ AIService inicializado correctamente');
    } catch (error) {
      console.error('‚ùå Error en inicializaci√≥n de AIService:', error);
      throw error;
    }
  }

  static async setConfig(configUpdate: Partial<AIConfig>): Promise<AIConfig> {
    try {
      console.log('üîÑ AIService.setConfig: Actualizando configuraci√≥n con:', {
        provider: configUpdate.provider,
        model: configUpdate.model,
        temperature: configUpdate.temperature,
        maxTokens: configUpdate.maxTokens,
        apiKey: configUpdate.apiKey ? '[REDACTED]' : null,
        // Omitiendo otros campos para claridad
      });

      // Si hay un cambio en el proveedor y una nueva API key, guardarla
      if (configUpdate.provider && configUpdate.apiKey) {
        console.log(`üîë Guardando API key para el proveedor ${configUpdate.provider}`);
        // Guardar la API key en la base de datos para este proveedor
        await this.saveApiKeyForProvider(configUpdate.provider as AIProvider, configUpdate.apiKey);
      }

      const response = await fetch('/api/ai-config', {
        method: 'PUT',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(configUpdate),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`Error ${response.status}: ${errorText}`);
      }

      const updatedConfig = await response.json();
      console.log('‚úÖ AIService.setConfig: Configuraci√≥n actualizada correctamente:', {
        provider: updatedConfig.provider,
        model: updatedConfig.model,
        temperature: updatedConfig.temperature,
        maxTokens: updatedConfig.maxTokens,
      });
      
      // Actualizamos la configuraci√≥n en memoria
      if (updatedConfig) {
        this.config = updatedConfig;
      }
      
      return updatedConfig;
    } catch (error) {
      console.error('‚ùå AIService.setConfig: Error:', error);
      throw error;
    }
  }
  
  /**
   * Guarda la API key para un proveedor espec√≠fico en la base de datos
   * @param provider El proveedor para el que se guarda la API key
   * @param apiKey La API key a guardar
   */
  private static async saveApiKeyForProvider(provider: AIProvider, apiKey: string): Promise<void> {
    try {
      console.log(`üíæ AIService.saveApiKeyForProvider: Guardando API key para ${provider}`);
      
      const response = await fetch('/api/ai-provider-key', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ provider, apiKey }),
      });
      
      if (!response.ok) {
        const errorText = await response.text();
        console.error(`‚ùå Error al guardar API key para ${provider}: ${response.status} ${errorText}`);
      } else {
        console.log(`‚úÖ API key guardada correctamente para ${provider}`);
      }
    } catch (error) {
      console.error(`‚ùå Error al guardar API key para ${provider}:`, error);
    }
  }

  static async getConfig(): Promise<AIConfig> {
    try {
      if (!this.config) {
        // Intentar cargar la configuraci√≥n desde la API
        const response = await fetch(this.getApiConfigUrl());
        if (!response.ok) {
          throw new Error('Error al obtener la configuraci√≥n de la API');
        }
        
        const dbConfig = await response.json();
        
        // Si no hay configuraci√≥n en la base de datos, crear una por defecto
        if (!dbConfig) {
          // Intentar obtener la API key del .env
          const envApiKey = process.env.NEXT_PUBLIC_GEMINI_API_KEY;
          
          if (!envApiKey) {
            console.warn('‚ö†Ô∏è No se encontr√≥ API key en las variables de entorno');
          }

          const defaultConfig: AIConfigInput = {
            provider: 'google',
            model: 'gemini-2.5-flash-preview-04-17',
            apiKey: envApiKey || null,
            temperature: 0.3,
            maxTokens: 30720,
            systemPrompt: null
          };

          console.log('Creando configuraci√≥n por defecto con API key del .env');

          const createResponse = await fetch(this.getApiConfigUrl(), {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
            },
            body: JSON.stringify(defaultConfig),
          });

          if (!createResponse.ok) {
            throw new Error('Error al crear la configuraci√≥n inicial');
          }

          const newConfig = await createResponse.json();
          this.config = newConfig;
        } else {
          // Si la configuraci√≥n existe pero no tiene API key, intentar usar la del .env
          if (!dbConfig.apiKey) {
            const envApiKey = process.env.NEXT_PUBLIC_GEMINI_API_KEY;
            
            if (envApiKey) {
              console.log('Actualizando configuraci√≥n con API key del .env');
              
              const updateResponse = await fetch(this.getApiConfigUrl(), {
                method: 'PUT',
                headers: {
                  'Content-Type': 'application/json',
                },
                body: JSON.stringify({
                  ...dbConfig,
                  apiKey: envApiKey
                }),
              });

              if (!updateResponse.ok) {
                throw new Error('Error al actualizar la configuraci√≥n con la API key del .env');
              }

              const updatedConfig = await updateResponse.json();
              this.config = updatedConfig;
            } else {
              this.config = dbConfig;
            }
          } else {
            this.config = dbConfig;
          }
        }
      }

      if (!this.config) {
        throw new Error('No se pudo inicializar la configuraci√≥n de IA');
      }

      return this.config;
    } catch (error) {
      console.error('Error al obtener la configuraci√≥n:', error);
      throw new Error('No se pudo obtener la configuraci√≥n de IA');
    }
  }

  static getSelectedModel(): AIModel | null {
    try {
      console.log('üîé getSelectedModel llamado. Configuraci√≥n actual:', 
        this.config ? { provider: this.config.provider, model: this.config.model } : 'No hay configuraci√≥n');
      
      if (!this.config || !this.config.provider || !this.config.model) {
        console.warn('‚ö†Ô∏è No hay configuraci√≥n o falta informaci√≥n del modelo/proveedor');
        
        // Si no hay configuraci√≥n, intentar cargar la configuraci√≥n m√°s reciente desde la API
        if (!this.isInitialized) {
          console.log('üìå AIService no est√° inicializado, inicializando...');
          // No podemos llamar directamente a initialize() aqu√≠ porque es async
          // y getSelectedModel() es s√≠ncrono, pero marcamos para inicializar pr√≥ximamente
          setTimeout(() => { this.initialize(); }, 0);
        }
        
        // Si no hay configuraci√≥n, devolver un modelo predeterminado
        if (!this.config) {
          console.log('üìå Usando modelo predeterminado de Google (√∫ltimo valor conocido)');
          return this.getDefaultModelForProvider('google' as AIProvider);
        }
        
        // Si hay configuraci√≥n pero falta el modelo o proveedor, intentar devolver un modelo predeterminado para el proveedor
        if (this.config.provider && !this.config.model) {
          console.log(`üìå Usando modelo predeterminado para ${this.config.provider}`);
          return this.getDefaultModelForProvider(this.config.provider as AIProvider);
        }
        
        return null;
      }
      
      const provider = this.config.provider as AIProvider;
      const modelId = this.config.model;
      
      // Primero buscar en el array global de modelos, que es lo que usa la p√°gina de AI Settings
      const globalModels = availableModels.filter(m => m.provider === provider);
      const selectedModelFromGlobal = globalModels.find(model => model.id === modelId);
      
      if (selectedModelFromGlobal) {
        console.log('‚úÖ Modelo seleccionado (desde array global):', selectedModelFromGlobal.name);
        return selectedModelFromGlobal;
      }
      
      // Si no se encuentra en el array global, buscar en el objeto de modelos disponibles
      const models = this.availableModels[provider] || [];
      const selectedModel = models.find(model => model.id === modelId);
      
      if (!selectedModel) {
        console.warn(`‚ö†Ô∏è Modelo ${modelId} no encontrado para el proveedor ${provider}`);
        
        // Si el modelo no se encuentra, devolver un modelo predeterminado para el proveedor
        console.log(`üìå Usando modelo predeterminado para ${provider}`);
        return this.getDefaultModelForProvider(provider);
      }
      
      console.log('‚úÖ Modelo seleccionado (desde objeto privado):', selectedModel.name);
      return selectedModel;
    } catch (error) {
      console.error('‚ùå Error en getSelectedModel:', error);
      return null;
    }
  }
  
  private static getApiEndpointForProvider(provider: AIProvider, modelId: string): string {
    switch(provider) {
      case 'google':
        return `https://generativelanguage.googleapis.com/v1beta/models/${modelId}:generateContent`;
      case 'openai':
        return 'https://api.openai.com/v1/chat/completions';
      case 'anthropic':
        return 'https://api.anthropic.com/v1/messages';
      case 'deepseek':
        return 'https://api.deepseek.com/v1/chat/completions';
      case 'xai':
        return 'https://api.x.ai/v1/chat/completions';
      case 'alibaba':
        return 'https://api.alibaba-inc.com/v1/chat/completions';
      default:
        return 'https://api.openai.com/v1/chat/completions';
    }
  }

  private static getApiKeyFromEnv(provider: AIProvider | string | undefined): string | null {
    if (!provider) {
      console.error('‚ùå Proveedor de IA no definido o inv√°lido en getApiKeyFromEnv');
      return null;
    }
    
    console.log(`üîç Buscando API key para ${provider} en variables de entorno`);
    
    // Mapa de providers a variables de entorno - PRIMERO las del servidor, luego cliente como fallback
    const envMap: Record<string, string | undefined> = {
      'openai': process.env.GPT_API_KEY || process.env.NEXT_PUBLIC_GPT_API_KEY,
      'anthropic': process.env.ANTHROPIC_API_KEY || process.env.NEXT_PUBLIC_ANTHROPIC_API_KEY,
      'google': process.env.GEMINI_API_KEY || process.env.NEXT_PUBLIC_GEMINI_API_KEY,
      'deepseek': process.env.DEEPSEEK_API_KEY || process.env.NEXT_PUBLIC_DEEPSEEK_API_KEY,
      'xai': process.env.XAI_API_KEY || process.env.GROK_API_KEY || process.env.NEXT_PUBLIC_GROK_API_KEY || process.env.NEXT_PUBLIC_XAI_API_KEY,
      'alibaba': process.env.QWEN_API_KEY || process.env.NEXT_PUBLIC_QWEN_API_KEY,
      'mistral': process.env.MISTRAL_API_KEY || process.env.NEXT_PUBLIC_MISTRAL_API_KEY,
      'cohere': process.env.COHERE_API_KEY || process.env.NEXT_PUBLIC_COHERE_API_KEY
    };
    
    const apiKey = envMap[provider];
    if (apiKey) {
      // Determinar si es server-side o client-side
      const isServerSide = apiKey === process.env.GPT_API_KEY || 
                          apiKey === process.env.ANTHROPIC_API_KEY || 
                          apiKey === process.env.GEMINI_API_KEY ||
                          apiKey === process.env.XAI_API_KEY ||
                          apiKey === process.env.GROK_API_KEY;
      console.log(`‚úÖ API key encontrada en variables de entorno para ${provider} (${isServerSide ? 'server-side' : 'client-side'})`);
      return apiKey;
    }
    
    console.warn(`‚ö†Ô∏è No se encontr√≥ API key para ${provider} en variables de entorno`);
    return null;
  }

  private static async getPrompt(promptName: string): Promise<string> {
    try {
      console.log(`üîÑ Obteniendo prompt ${promptName}...`);
      
      // Obtener el prompt usando PromptService
      const prompt = await PromptService.getPrompt(promptName);
      
      if (prompt) {
        console.log(`‚úÖ Prompt ${promptName} obtenido correctamente`);
        return prompt.content;
      }
      
      throw new Error(`Prompt ${promptName} no encontrado`);
    } catch (error) {
      console.error(`‚ùå Error al obtener el prompt ${promptName}:`, error);
      throw new Error(`Error al obtener el prompt ${promptName}`);
    }
  }

  /**
   * Calcula aproximadamente el n√∫mero de tokens en un texto
   * Aproximaci√≥n: 1 token ‚âà 4 caracteres en espa√±ol
   */
  private static estimateTokens(text: string): number {
    return Math.ceil(text.length / 4);
  }

  /**
   * Trunca el contenido del documento si es necesario para que el prompt completo quepa en el l√≠mite de tokens
   */
  private static truncateContentForTokenLimit(content: string, basePromptTokens: number, maxTokens: number, maxOutputTokens: number = 2000): string {
    const availableTokensForContent = maxTokens - basePromptTokens - maxOutputTokens;
    const contentTokens = this.estimateTokens(content);
    
    console.log(`üìä Token analysis:`, {
      basePromptTokens,
      contentTokens,
      maxTokens,
      maxOutputTokens,
      availableTokensForContent
    });

    if (contentTokens <= availableTokensForContent) {
      return content;
    }

    // Truncar el contenido manteniendo p√°rrafos completos
    const targetLength = Math.floor(availableTokensForContent * 4 * 0.9); // 90% del l√≠mite para margen de seguridad
    const paragraphs = content.split('\n\n');
    let truncatedContent = '';
    
    for (const paragraph of paragraphs) {
      if ((truncatedContent + paragraph).length > targetLength) {
        break;
      }
      truncatedContent += (truncatedContent ? '\n\n' : '') + paragraph;
    }

    console.log(`‚ö†Ô∏è Contenido truncado de ${content.length} a ${truncatedContent.length} caracteres`);
    return truncatedContent + '\n\n[... contenido truncado para ajustar l√≠mite de tokens ...]';
  }

  /**
   * Genera un prompt optimizado que respeta el l√≠mite de tokens del modelo
   */
  private static async generateOptimizedPrompt(config: AIConfig, document: Document, numberOfQuestions: number, optionLength?: OptionLengthType, normativa?: string, maxTokens: number = 8192): Promise<string> {
    try {
      console.log('üîÑ Generando prompt optimizado...', { numberOfQuestions, maxTokens });
      
      // üéØ FIX: Usar el t√≠tulo personalizado si est√° disponible
      const tituloFormato = normativa 
        ? `<b>${normativa}</b><br><br>`
        : `<b>T√çTULO DE LA PREGUNTA</b><br><br>`;
      
      // Prompt base m√≠nimo (sin ejemplos extensos)
      const basePrompt = `
Genera preguntas tipo test en formato GIFT para Moodle siguiendo EXACTAMENTE esta estructura:

**FORMATO OBLIGATORIO:**
${tituloFormato}
Texto de la pregunta {
=opci√≥n correcta
~%-33.33333%opci√≥n incorrecta 1
~%-33.33333%opci√≥n incorrecta 2  
~%-33.33333%opci√≥n incorrecta 3
#### RETROALIMENTACI√ìN:<br><br>
[Explicaci√≥n detallada de la respuesta correcta]
}

**REGLAS ESTRICTAS:**
- Respuesta correcta: =
- Respuestas incorrectas: ~%-33.33333%
- T√≠tulo siempre en <b></b><br><br>
- Retroalimentaci√≥n siempre con #### RETROALIMENTACI√ìN:<br><br>
- 4 opciones total (1 correcta + 3 incorrectas)
- Usar HTML para formato cuando sea necesario

${normativa ? `**INSTRUCCI√ìN ESPEC√çFICA DE T√çTULO:**
OBLIGATORIO: En cada pregunta, usa "${normativa}" como t√≠tulo exacto.

` : ''}Genera ${numberOfQuestions} preguntas basadas en el siguiente contenido:

${document.content}
      `;

      const baseTokens = this.estimateTokens(basePrompt);
      const optimizedContent = this.truncateContentForTokenLimit(document.content, baseTokens, maxTokens);
      
      return basePrompt + optimizedContent;
      
    } catch (error) {
      console.error('‚ùå Error al generar prompt optimizado:', error);
      throw new Error('No se pudo generar el prompt optimizado');
    }
  }

  private static async generatePrompt(config: AIConfig, document: Document, numberOfQuestions: number, optionLength?: OptionLengthType, normativa?: string): Promise<string> {
    try {
      console.log('üîÑ Generando prompt con configuraci√≥n...', { ...config, numberOfQuestions });
      
      const [
        expertPromptContent,
        rawFormatPromptContent,
        difficultyPromptContent,
        documentationPromptContent,
        qualityPromptContent
      ] = await Promise.all([
        this.getPrompt('expertPrompt'),
        this.getPrompt('formatPrompt'),
        this.getPrompt('difficultyPrompt'),
        this.getPrompt('documentationPrompt'),
        this.getPrompt('qualityPrompt')
      ]);
      
      const formatPromptInterpolated = normativa
        ? rawFormatPromptContent.replace(/\[NOMBRE DE LA NORMA SEG√öN EL TEXTO FUENTE\]/g, normativa)
        : rawFormatPromptContent;

      // Ya no necesitamos isRegimenDisciplinarioDocument
      // selectDistractorsPrompt ahora siempre devuelve el prompt general
      const distractorsPromptSelected = PromptSelectorService.selectDistractorsPrompt(
        document.title, // Todav√≠a se pueden pasar por si en el futuro se reutilizan
        document.content
      );
      
      console.log('üìù Prompt de distractores seleccionado: distractorsPrompt (general)');
      console.log('üìù Contenido del prompt de distractores (primeras l√≠neas):', 
        distractorsPromptSelected.split('\n').slice(0, 3).join('\n')
      );

      const optionLengthInstruction = getOptionLengthInstruction(optionLength);

      // Usar prompt espec√≠fico de Telegram si se selecciona esa opci√≥n
      const telegramSpecificPrompt = optionLength === 'telegram' ? `
${telegramPrompt}
` : '';

      // Instrucci√≥n expl√≠cita y ejemplo oficial para reforzar el estilo sencillo y claro
      const instruccionesEstilo = optionLength === 'telegram' ? telegramSpecificPrompt : `
IMPORTANTE: Formula las preguntas de manera sencilla, clara y directa, como en los ex√°menes oficiales. El enunciado debe ser breve, f√°cil de entender y referenciar la norma o art√≠culo correspondiente si aplica. NO uses frases complejas ni introducciones largas. Sigue el formato exacto indicado a continuaci√≥n.
`;

      // Ejemplo reducido para ahorrar tokens
      const ejemploOficial = `
// EJEMPLO DE PREGUNTA OFICIAL
<b>Instrucci√≥n 55/2021, de 27 de octubre, del Jefe de Estado Mayor de la Defensa.</b><br><br>
La Unidad de Verificaci√≥n (UVE) tendr√° como responsabilidad: {
=el desarme, el control de armamentos y el establecimiento de medidas de fomento de la confianza y seguridad
~%-33.33333%la gesti√≥n de la ayuda humanitaria y la respuesta ante cat√°strofes naturales
~%-33.33333%el control de armamentos y la verificaci√≥n de su cumplimiento por otros Estados
~%-33.33333%la planificaci√≥n operativa conjunta y el desarrollo de la doctrina
#### RETROALIMENTACI√ìN:<br><br>
<b>Referencia:</b> "La UVE tendr√° como responsabilidad... compromisos internacionales relacionados con <b>el desarme, el control de armamentos y el establecimiento de medidas de fomento de la confianza y seguridad</b>."<br>
Explicaci√≥n breve del fundamento legal.<br>
}
`;

      // Instrucciones espec√≠ficas para el t√≠tulo personalizado
      const titleInstruction = normativa 
        ? `\nINSTRUCCI√ìN ESPEC√çFICA DE T√çTULO:
OBLIGATORIO: En cada pregunta, usa "${normativa}" como t√≠tulo.
`
        : '';

      // Construir el prompt completo
      const fullPrompt = `
${instruccionesEstilo}
${ejemploOficial}

FORMATO:
${formatPromptInterpolated}

${titleInstruction}

DIFICULTAD:
${difficultyPromptContent}

DISTRACTORES:
${distractorsPromptSelected}

DOCUMENTACI√ìN:
${documentationPromptContent}

CALIDAD:
${qualityPromptContent}

${optionLengthInstruction}

N√öMERO DE PREGUNTAS:
OBLIGATORIO: Generar EXACTAMENTE ${numberOfQuestions} preguntas.

DOCUMENTO A PROCESAR:
${document.content}

CONFIGURACI√ìN:
${JSON.stringify({provider: config.provider, model: config.model, numberOfQuestions}, null, 2)}
`;

      // Verificar tama√±o del prompt
      const promptTokens = this.estimateTokens(fullPrompt);
      console.log(`üìä Prompt generado: ~${promptTokens} tokens estimados`);

      return fullPrompt;
    } catch (error) {
      console.error('‚ùå Error al generar el prompt:', error);
      throw new Error('No se pudieron cargar los prompts necesarios');
    }
  }

  private static generateInstructions(numPreguntas: number, isLegalText: boolean, config: GenerateQuestionsConfig): string {
    // Verificar si hay configuraciones avanzadas en el objeto config
    const useLegalText = isLegalText || (config.advancedFeatures?.legalTextProcessing === true);
    const useConceptTrap = this.useConceptTrap || (config.advancedFeatures?.conceptualTrap === true);
    const usePrecisionDistractors = this.usePrecisionDistractors || (config.advancedFeatures?.precisionDistractors === true);
    
    console.log('üîÑ Generando instrucciones con configuraci√≥n:', {
      textoLegal: useLegalText ? '‚úÖ Activado' : '‚ùå No activado',
      trampaConceptual: useConceptTrap ? '‚úÖ Activada' : '‚ùå No activada',
      distractoresPrecision: usePrecisionDistractors ? '‚úÖ Activados' : '‚ùå No activados'
    });

    const legalTextInstructions = useLegalText ? `
    INSTRUCCIONES ESPEC√çFICAS PARA TEXTO LEGAL:
    1. OBLIGATORIO: Genera preguntas SIGUIENDO EL ORDEN de los art√≠culos en el texto.
    2. OBLIGATORIO: Cada pregunta debe estar basada en un art√≠culo espec√≠fico.
    3. OBLIGATORIO: Respeta la progresi√≥n natural de los art√≠culos.
    4. OBLIGATORIO: Incluye el n√∫mero del art√≠culo en el t√≠tulo de la pregunta.
    5. OBLIGATORIO: Cita textualmente el art√≠culo relevante en la retroalimentaci√≥n.
    ` : '';

    const conceptTrapInstructions = useConceptTrap ? `
    INSTRUCCIONES PARA TRAMPAS CONCEPTUALES:
    1. OBLIGATORIO: Al menos 2 de las 3 opciones incorrectas deben ser "trampas conceptuales".
    2. OBLIGATORIO: Las trampas conceptuales deben usar terminolog√≠a correcta del texto, pero aplicada incorrectamente.
    3. OBLIGATORIO: Las trampas conceptuales deben parecer plausibles para estudiantes que han memorizado t√©rminos sin entender conceptos.
    ` : '';

    const precisionDistractorsInstructions = usePrecisionDistractors ? `
    INSTRUCCIONES PARA DISTRACTORES DE PRECISI√ìN:
    1. OBLIGATORIO: Crea distractores que contengan variaciones sutiles pero significativas.
    2. OBLIGATORIO: Aplica t√©cnicas de precisi√≥n en los distractores.
    3. OBLIGATORIO: Estos distractores deben parecer correctos a quien no domina los detalles exactos.
    ` : '';

    return `
    ${legalTextInstructions}
    ${conceptTrapInstructions}
    ${precisionDistractorsInstructions}
    
    IMPORTANTE:
    1. OBLIGATORIO: Genera EXACTAMENTE ${numPreguntas} preguntas.
    2. OBLIGATORIO: Sigue ESTRICTAMENTE el formato GIFT especificado.
    3. OBLIGATORIO: Incluye TODAS las etiquetas HTML y emojis requeridos.
    4. OBLIGATORIO: Proporciona retroalimentaci√≥n detallada para cada pregunta.
    5. OBLIGATORIO: Las respuestas incorrectas deben tener una longitud y complejidad similar a la respuesta correcta:
       - Cada respuesta incorrecta debe ser tan detallada y elaborada como la correcta
       - Evita respuestas incorrectas demasiado cortas o simplistas
       - Mant√©n un nivel de detalle y especificidad consistente en todas las opciones
       - Las respuestas incorrectas deben ser plausibles y bien construidas
    `;
  }

  private static splitContentIntoChunks(content: string, chunkSize: number): string[] {
    // Estimaci√≥n aproximada: 1 token ‚âà 4 caracteres
    const MAX_CHUNK_LENGTH = 6000; // Dejamos margen para el prompt y otros elementos
    const chunks: string[] = [];
    
    // Dividir por p√°rrafos
    const paragraphs = content.split('\n\n');
    let currentChunk = '';
    
    for (const paragraph of paragraphs) {
      // Si el p√°rrafo por s√≠ solo excede el l√≠mite, subdiv√≠delo
      if (paragraph.length > MAX_CHUNK_LENGTH) {
        const sentences = paragraph.split(/[.!?]+/);
        let tempChunk = '';
        
        for (const sentence of sentences) {
          if ((tempChunk + sentence).length > MAX_CHUNK_LENGTH) {
            if (tempChunk) chunks.push(tempChunk.trim());
            tempChunk = sentence;
          } else {
            tempChunk += sentence + '. ';
          }
        }
        if (tempChunk) chunks.push(tempChunk.trim());
        continue;
      }
      
      // Agregar p√°rrafo al chunk actual o crear uno nuevo
      if ((currentChunk + paragraph).length > MAX_CHUNK_LENGTH) {
        if (currentChunk) chunks.push(currentChunk.trim());
        currentChunk = paragraph;
      } else {
        currentChunk += (currentChunk ? '\n\n' : '') + paragraph;
      }
    }
    
    // Agregar el √∫ltimo chunk si existe
    if (currentChunk) chunks.push(currentChunk.trim());
    
    console.log(`üìä Texto dividido en ${chunks.length} fragmentos`);
    chunks.forEach((chunk, i) => {
      console.log(`Fragmento ${i + 1}: ${Math.round(chunk.length / 4)} tokens estimados`);
    });
    
    return chunks;
  }

  private static async processContentInChunks(content: string, maxChunkSize: number = 4000): Promise<string[]> {
    // Dividir el contenido en p√°rrafos
    const paragraphs = content.split('\n\n');
    const chunks: string[] = [];
    let currentChunk = '';

    for (const paragraph of paragraphs) {
      // Si el p√°rrafo actual m√°s el siguiente exceder√≠an el tama√±o m√°ximo
      if ((currentChunk + paragraph).length > maxChunkSize && currentChunk.length > 0) {
        chunks.push(currentChunk.trim());
        currentChunk = paragraph;
      } else {
        currentChunk = currentChunk ? `${currentChunk}\n\n${paragraph}` : paragraph;
      }
    }

    // A√±adir el √∫ltimo chunk si existe
    if (currentChunk) {
      chunks.push(currentChunk.trim());
    }

    return chunks;
  }

  public static async generateQuestions(
    content: string,
    numberOfQuestions: number = 5,
    questionTypeCounts?: Record<string, number>,
    optionLength?: OptionLengthType,
    modelOverride?: AIModel, // Nuevo argumento opcional
    normativa?: string // Nuevo argumento para la norma
  ): Promise<string> {
    try {
      if (!this.config) {
        throw new Error('AIService no inicializado');
      }

      if (!content) {
        throw new Error('No se proporcion√≥ contenido para generar preguntas');
      }

      // Validar el n√∫mero de preguntas
      if (numberOfQuestions < 1 || numberOfQuestions > 500) {
        throw new Error('El n√∫mero de preguntas debe estar entre 1 y 500');
      }

      // Usar el modelo pasado como override, si existe
      const selectedModel = modelOverride || this.getSelectedModel();
      if (!selectedModel) {
        throw new Error('No se ha seleccionado ning√∫n modelo de IA');
      }
      
      // Obtener la API key para el proveedor seleccionado
      console.log(`üîë Obteniendo API key para proveedor: ${selectedModel.provider}`);
      const apiKey = this.getApiKeyFromEnv(selectedModel.provider);
      
      if (!apiKey) {
        console.error(`‚ùå No se encontr√≥ API key para el proveedor ${selectedModel.provider}`);
        throw new Error(`No se encontr√≥ API key para el proveedor ${selectedModel.provider}`);
      }
      
      console.log(`‚úÖ API key obtenida para ${selectedModel.provider}, generando preguntas...`);

      // --- NUEVO PROMPT DE TIPOS DE PREGUNTA ---
      let typeInstruction = '';
      if (questionTypeCounts && Object.values(questionTypeCounts).some(v => v > 0)) {
        const typeLabels: Record<string, string> = {
          textual: 'preguntas textuales',
          blank_spaces: 'preguntas de espacios en blanco',
          identify_incorrect: 'preguntas de identificar la incorrecta',
          none_correct: 'preguntas de ninguna es correcta',
        };
        const parts = Object.entries(questionTypeCounts)
          .filter(([, v]) => v > 0)
          .map(([k, v]) => `${v} ${typeLabels[k] || k}`);
        typeInstruction = `IMPORTANTE: Genera exactamente ${parts.join(', ')}. No generes m√°s ni menos de cada tipo. Cada pregunta debe estar claramente identificada por su tipo.`;
      }
      // --- FIN NUEVO PROMPT ---

      // --- INCLUYO SIEMPRE EL PROMPT DE FORMATO Y UNA INSTRUCCI√ìN EXTRA ---
      const formatReinforcement = `Cada pregunta debe tener exactamente 4 opciones: 1 correcta y 3 incorrectas. No expliques los distractores en las opciones. Sigue el formato de ejemplo.`;

      // Determinar el n√∫mero de preguntas por chunk
      const questionsPerChunk = this.config.questionsPerChunk || 5;
      const chunks = await this.processContentInChunks(content);
      let allQuestions: string[] = [];

      console.log(`Procesando ${chunks.length} chunks de contenido...`);

      // Distribuir el n√∫mero de preguntas entre los chunks
      const questionsPerChunkArray = this.distributeQuestions(numberOfQuestions, chunks.length, questionsPerChunk);

      // Palabras prohibidas para filtrar - ACTUALIZADO: lista m√°s espec√≠fica
      // Evita t√©rminos absolutos obvios pero permite terminolog√≠a t√©cnica v√°lida
      const forbiddenWords = [
        '√∫nicamente.*respuesta.*correcta', // Frases obvias
        'exclusivamente.*opci√≥n.*v√°lida',
        'todas.*anteriores.*correctas',
        'ninguna.*anteriores.*correcta',
        'nada m√°s que.*opci√≥n',
        'obviamente.*respuesta'
      ];
      const forbiddenRegex = new RegExp(forbiddenWords.join('|'), 'i');

      for (let i = 0; i < chunks.length; i++) {
        const chunk = chunks[i];
        const questionsForThisChunk = questionsPerChunkArray[i];

        if (questionsForThisChunk === 0) continue;

        const document = {
          id: crypto.randomUUID(),
          title: `Chunk ${i + 1}/${chunks.length}`,
          content: chunk,
          date: new Date(),
          type: 'text/plain'
        };

        // Generar prompt y verificar l√≠mite de tokens
        let prompt = '';
        let useOptimizedPrompt = false;
        
        try {
          // Primero intentar con el prompt completo
          prompt = await this.generatePrompt(this.config, document, questionsForThisChunk, optionLength, normativa);
          const promptTokens = this.estimateTokens(prompt);
          const maxTokens = selectedModel.config.maxTokens;
          
          // Si excede ~70% del l√≠mite, usar prompt optimizado
          if (promptTokens > maxTokens * 0.7) {
            console.log(`‚ö†Ô∏è Prompt demasiado largo (${promptTokens} tokens), usando versi√≥n optimizada...`);
            prompt = await this.generateOptimizedPrompt(this.config, document, questionsForThisChunk, optionLength, normativa, maxTokens);
            useOptimizedPrompt = true;
          }
        } catch (error) {
          console.error('‚ùå Error generando prompt:', error);
          // Fallback al prompt optimizado
          prompt = await this.generateOptimizedPrompt(this.config, document, questionsForThisChunk, optionLength, normativa);
          useOptimizedPrompt = true;
        }

        let response = '';
        let questions: string[] = [];
        let attempts = 0;
        
        do {
          try {
            response = await this.callAIService(selectedModel, prompt, apiKey);
            
            // üîç DEBUG: Mostrar respuesta de la IA para diagn√≥stico
            console.log('üîç [DEBUG] Respuesta completa de la IA:', response.substring(0, 500) + (response.length > 500 ? '...' : ''));
            
            // ‚úÖ ACTUALIZADO: Buscar preguntas en formato GIFT sin comentarios
            // Antes: buscaba '::' en los comentarios eliminados
            // Ahora: busca el patr√≥n GIFT: texto que termine con '{'
            const allSplitParts = response.split('\n\n');
            console.log('üîç [DEBUG] Partes despu√©s de split por \\n\\n:', allSplitParts.length);
            allSplitParts.forEach((part, index) => {
              console.log(`üîç [DEBUG] Parte ${index}: "${part.substring(0, 100)}${part.length > 100 ? '...' : ''}"`);
            });
            
            questions = allSplitParts.filter(q => {
              const trimmed = q.trim();
              const hasContent = trimmed && (trimmed.includes('{') || trimmed.includes('<b>'));
              console.log(`üîç [DEBUG] Filtro - "${trimmed.substring(0, 50)}..." -> ${hasContent ? 'V√ÅLIDA' : 'DESCARTADA'}`);
              return hasContent;
            });
            
            console.log(`üîç [DEBUG] Preguntas filtradas: ${questions.length}`);
            
            // Si alguna opci√≥n contiene palabras prohibidas, pide regenerar
            const hasForbidden = questions.some(q => forbiddenRegex.test(q));
            if (!hasForbidden) break;
            attempts++;
            console.warn('‚ö†Ô∏è Pregunta(s) con palabras prohibidas detectada(s). Reintentando generaci√≥n...');
          } catch (error: any) {
            console.error(`‚ùå Error en intento ${attempts + 1}:`, error.message);
            
            // Si es un error de l√≠mite de tokens y no hemos usado el prompt optimizado, usarlo
            if (error.message.includes('maximum context length') && !useOptimizedPrompt) {
              console.log('üîÑ Cambiando a prompt optimizado por error de tokens...');
              prompt = await this.generateOptimizedPrompt(this.config, document, questionsForThisChunk, optionLength, normativa);
              useOptimizedPrompt = true;
              attempts--; // No contar este intento
              continue;
            }
            
            attempts++;
            if (attempts >= 3) {
              throw error;
            }
          }
        } while (attempts < 3);

        // Filtrar preguntas v√°lidas
        questions = questions.filter(q => !forbiddenRegex.test(q));
        allQuestions = [...allQuestions, ...questions];
      }

      // Verificar si tenemos el n√∫mero correcto de preguntas
      if (allQuestions.length !== numberOfQuestions) {
        console.warn(`‚ö†Ô∏è Se generaron ${allQuestions.length} preguntas en lugar de ${numberOfQuestions} solicitadas`);
        if (allQuestions.length < numberOfQuestions) {
          const remainingQuestions = numberOfQuestions - allQuestions.length;
          console.log(`Intentando generar ${remainingQuestions} preguntas adicionales...`);
          
          try {
            const additionalPrompt = await this.generateOptimizedPrompt(this.config, {
              id: crypto.randomUUID(),
              title: 'Preguntas adicionales',
              content: `${content}\n\nNOTA: Generar ${remainingQuestions} preguntas ADICIONALES diferentes a las anteriores.`,
              date: new Date(),
              type: 'text/plain'
            }, remainingQuestions, optionLength, normativa);

            let additionalResponse = '';
            let newQuestions: string[] = [];
            let attempts = 0;
            do {
              additionalResponse = await this.callAIService(selectedModel, additionalPrompt, apiKey);
              // ‚úÖ ACTUALIZADO: Buscar preguntas adicionales en formato GIFT sin comentarios
              // Antes: buscaba '::' en los comentarios eliminados
              // Ahora: busca el patr√≥n GIFT: texto que termine con '{'
              newQuestions = additionalResponse.split('\n\n').filter(q => {
                const trimmed = q.trim();
                return trimmed && (trimmed.includes('{') || trimmed.includes('<b>'));
              });
              const hasForbidden = newQuestions.some(q => forbiddenRegex.test(q));
              if (!hasForbidden) break;
              attempts++;
              console.warn('‚ö†Ô∏è Pregunta(s) adicional(es) con palabras prohibidas detectada(s). Reintentando generaci√≥n...');
            } while (attempts < 3);

            newQuestions = newQuestions.filter(q => !forbiddenRegex.test(q));
            allQuestions = [...allQuestions, ...newQuestions];
          } catch (error) {
            console.error('‚ùå Error generando preguntas adicionales:', error);
            // Continuar con las preguntas que tenemos
          }
        }
      }

      // Asegurar que no excedemos el n√∫mero de preguntas solicitado
      // Paso 1: Clasificar preguntas por tipo (heur√≠stica simple)
      const typeConfig = require('./questionGeneratorService').questionTypes;
      const classifiedQuestions = allQuestions.map((q: string) => {
        const lower = q.toLowerCase();
        if (lower.includes('rellena el espacio') || lower.includes('completar')) return { type: 'blank_spaces', content: q };
        if (lower.includes('ninguna es correcta')) return { type: 'none_correct', content: q };
        if (lower.includes('incorrecta') || lower.includes('no es correcta')) return { type: 'identify_incorrect', content: q };
        // Por defecto, textual
        return { type: 'textual', content: q };
      });

      // Paso 2: Seleccionar la cantidad exacta de cada tipo
      const { selectQuestionsByType } = require('./questionGeneratorService');
      let seleccionadas: { type: string; content: string }[];
      if (questionTypeCounts) {
        // Selecci√≥n exacta por tipo
        seleccionadas = [];
        for (const [type, count] of Object.entries(questionTypeCounts)) {
          if (count > 0) {
            const disponibles = classifiedQuestions.filter((q: { type: string; content: string }) => q.type === type);
            seleccionadas.push(...disponibles.slice(0, count));
          }
        }
        // Si faltan preguntas, rellenar con cualquier tipo
        if (seleccionadas.length < numberOfQuestions) {
          const resto = classifiedQuestions.filter((q: { type: string; content: string }) => !seleccionadas.includes(q));
          seleccionadas.push(...resto.slice(0, numberOfQuestions - seleccionadas.length));
        }
        seleccionadas = seleccionadas.slice(0, numberOfQuestions);
      } else {
        seleccionadas = selectQuestionsByType(classifiedQuestions, typeConfig, numberOfQuestions);
      }

      // Paso 3: Devolver como string GIFT
      return seleccionadas.map((q: { type: string; content: string }) => q.content).join('\n\n');
    } catch (error) {
      console.error('Error en la generaci√≥n de preguntas:', error);
      throw error;
    }
  }

  private static distributeQuestions(totalquestions: number, numChunks: number, maxPerChunk: number): number[] {
    const distribution = new Array(numChunks).fill(0);
    let remainingQuestions = totalquestions;
    
    // Distribuir preguntas equitativamente
    const baseQuestions = Math.min(Math.floor(totalquestions / numChunks), maxPerChunk);
    distribution.fill(baseQuestions);
    remainingQuestions -= baseQuestions * numChunks;

    // Distribuir preguntas restantes
    let chunkIndex = 0;
    while (remainingQuestions > 0 && chunkIndex < numChunks) {
      if (distribution[chunkIndex] < maxPerChunk) {
        distribution[chunkIndex]++;
        remainingQuestions--;
      }
      chunkIndex++;
    }

    return distribution;
  }

  private static async callAIService(model: AIModel, prompt: string, apiKey: string): Promise<string> {
    try {
      switch (model.provider) {
        case 'google':
          return await this.callGoogleAI(model, prompt, apiKey);
        case 'openai':
          return await this.callOpenAI(model, prompt, apiKey);
        case 'anthropic':
          return await this.callAnthropic(model, prompt, apiKey);
        case 'xai':
          return await this.callXaiAI(model, prompt, apiKey);
        default:
          throw new Error(`Proveedor no soportado: ${model.provider}`);
      }
    } catch (error) {
      console.error(`Error al llamar al servicio de IA (${model.provider}):`, error);
      throw error;
    }
  }

  private static async callGoogleAI(model: AIModel, prompt: string, apiKey: string): Promise<string> {
    const endpoint = `${model.config.apiEndpoint}?key=${apiKey}`;
    
    const response = await fetch(endpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        contents: [{
          parts: [{
            text: prompt
          }]
        }],
        generationConfig: {
          temperature: this.config?.temperature || model.config.temperature,
          maxOutputTokens: this.config?.maxTokens || model.config.maxTokens
        }
      })
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`Error de Google AI: ${error.error?.message || 'Error desconocido'}`);
    }

    const data = await response.json();
    
    // Verificar que la respuesta tenga la estructura esperada
    if (!data.candidates || !Array.isArray(data.candidates) || data.candidates.length === 0) {
      console.error('‚ùå Respuesta de Google AI sin candidatos:', data);
      throw new Error('Error de Google AI: Respuesta sin candidatos v√°lidos');
    }

    const candidate = data.candidates[0];
    
    if (!candidate.content || !candidate.content.parts || !Array.isArray(candidate.content.parts) || candidate.content.parts.length === 0) {
      console.error('‚ùå Respuesta de Google AI sin contenido v√°lido:', candidate);
      throw new Error('Error de Google AI: Respuesta sin contenido v√°lido');
    }

    const part = candidate.content.parts[0];
    
    if (!part.text || typeof part.text !== 'string') {
      console.error('‚ùå Respuesta de Google AI sin texto v√°lido:', part);
      throw new Error('Error de Google AI: Respuesta sin texto v√°lido');
    }

    return part.text;
  }

  private static async callOpenAI(model: AIModel, prompt: string, apiKey: string): Promise<string> {
    try {
      // Ajustar max_tokens seg√∫n el modelo
      const maxAllowedTokens = model.config.maxTokens;
      const requestedMaxTokens = this.config?.maxTokens || model.config.maxTokens;
      const max_tokens = Math.min(requestedMaxTokens, maxAllowedTokens);

      const response = await fetch(model.config.apiEndpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${apiKey}`
        },
        body: JSON.stringify({
          model: model.config.modelId,
          messages: [
            {
              role: 'system',
              content: 'Eres un experto en generar preguntas de examen en formato GIFT. Sigues estrictamente las instrucciones proporcionadas.'
            },
            {
              role: 'user',
              content: prompt
            }
          ],
          temperature: this.config?.temperature || model.config.temperature,
          max_tokens: max_tokens
        })
      });

      if (!response.ok) {
        const error = await response.json();
        throw new Error(`Error de OpenAI: ${error.error?.message || 'Error desconocido'}`);
      }

      const data = await response.json();
      return data.choices[0].message.content;
    } catch (error) {
      console.error('Error al llamar al servicio de OpenAI:', error);
      throw error;
    }
  }

  private static async callAnthropic(model: AIModel, prompt: string, apiKey: string): Promise<string> {
    try {
      // Determinar la URL absoluta o relativa seg√∫n entorno
      const isServer = typeof window === 'undefined';
      const baseUrl = isServer ? process.env.NEXT_PUBLIC_BASE_URL || 'http://localhost:3000' : '';
      const endpoint = isServer ? `${baseUrl}/api/anthropic-proxy` : '/api/anthropic-proxy';
      // Llamada al endpoint proxy backend
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          prompt,
          model: model.config.modelId,
          max_tokens: this.config?.maxTokens || model.config.maxTokens
        })
      });
      if (!response.ok) {
        let errorMsg = 'Error desconocido';
        try {
          const error = await response.json();
          errorMsg = error.error || JSON.stringify(error);
        } catch (e) {
          errorMsg = await response.text();
        }
        throw new Error(`Error de Anthropic (proxy): ${errorMsg}`);
      }
      const data = await response.json();
      // Claude v3 responde con 'content' como array de objetos {type, text}
      if (data?.content) {
        if (Array.isArray(data.content)) {
          return data.content.map((c: any) => c.text).join('');
        }
        return data.content;
      }
      if (data?.choices?.[0]?.message?.content) {
        return data.choices[0].message.content;
      }
      throw new Error('Respuesta inesperada de Anthropic (proxy)');
    } catch (error) {
      console.error('Error al llamar al servicio de Anthropic (proxy):', error);
      throw error;
    }
  }

  /**
   * Llama al endpoint oficial de xAI/Grok para chat completions.
   * Documentaci√≥n: https://docs.x.ai/docs/api-reference
   * El formato es similar a OpenAI, pero requiere el header 'Authorization: Bearer <apiKey>' y 'Content-Type: application/json'.
   */
  private static async callXaiAI(model: AIModel, prompt: string, apiKey: string): Promise<string> {
    const response = await fetch(model.config.apiEndpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${apiKey}`
      },
      body: JSON.stringify({
        model: model.config.modelId,
        messages: [
          {
            role: 'system',
            content: 'Eres un experto en generar preguntas de examen en formato GIFT. Sigues estrictamente las instrucciones proporcionadas.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: this.config?.temperature || model.config.temperature,
        max_tokens: this.config?.maxTokens || model.config.maxTokens
      })
    });

    if (!response.ok) {
      const error = await response.json();
      throw new Error(`Error de xAI/Grok: ${error.error?.message || 'Error desconocido'}`);
    }

    const data = await response.json();
    // El formato de respuesta es similar a OpenAI
    return data.choices[0].message.content;
  }

  // Getters y setters para caracter√≠sticas
  static async enableConceptTrap(): Promise<void> {
    this.useConceptTrap = true;
    await this.enableFeature('conceptTrap');
  }

  static async disableConceptTrap(): Promise<void> {
    this.useConceptTrap = false;
    await this.disableFeature('conceptTrap');
  }

  static async enablePrecisionDistractors(): Promise<void> {
    this.usePrecisionDistractors = true;
    await this.enableFeature('precisionDistractors');
  }

  static async disablePrecisionDistractors(): Promise<void> {
    this.usePrecisionDistractors = false;
    await this.disableFeature('precisionDistractors');
  }

  static async loadFeatures(): Promise<void> {
    try {
      console.log('üîÑ Cargando caracter√≠sticas...');
      const features = await this.getFeatures();
      this.useConceptTrap = features.conceptTrap;
      this.usePrecisionDistractors = features.precisionDistractors;
      console.log('‚úÖ Caracter√≠sticas cargadas:', {
        'Trampas Conceptuales': features.conceptTrap ? '‚úÖ Activado' : '‚ùå Desactivado',
        'Distractores de Precisi√≥n': features.precisionDistractors ? '‚úÖ Activado' : '‚ùå Desactivado'
      });
    } catch (error) {
      console.error('‚ùå Error al cargar caracter√≠sticas:', error);
      throw error;
    }
  }

  // Funci√≥n para establecer directamente la API key
  static async setProviderApiKey(provider: AIProvider, apiKey: string): Promise<void> {
    console.log(`üîÑ Configurando API key de ${provider}...`);
    
    try {
      // Obtener la configuraci√≥n actual de la base de datos
      const configResponse = await fetch(this.getApiConfigUrl());
      const currentConfig = await configResponse.json();
      
      console.log('üì• Configuraci√≥n actual:', {
        provider: currentConfig?.provider,
        model: currentConfig?.model
      });

      // Verificar primero si la API key es v√°lida
      console.log('üîë Verificando nueva API key...');
      const verifyResponse = await fetch('/api/ai-config/verify', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          provider: provider,
          apiKey: apiKey
        }),
      });

      if (!verifyResponse.ok) {
        const errorData = await verifyResponse.json().catch(() => ({}));
        console.error('‚ùå API key inv√°lida:', errorData);
        throw new Error('API key inv√°lida');
      }

      console.log('‚úÖ API key verificada correctamente');

      // Mantener el modelo actual si el proveedor es el mismo, si no, usar el modelo por defecto
      let modelToUse;
      if (currentConfig?.provider === provider && currentConfig?.model) {
        modelToUse = currentConfig.model;
        console.log('üìù Manteniendo modelo actual:', modelToUse);
      } else {
        const defaultModel = this.getDefaultModelForProvider(provider);
        modelToUse = defaultModel.id;
        console.log('üìù Usando modelo por defecto:', modelToUse);
      }

      // Preparar la configuraci√≥n a guardar
      const configToSave = {
        provider: provider,
        model: modelToUse,
        apiKey: apiKey,
        temperature: currentConfig?.temperature || 0.3,
        maxTokens: currentConfig?.maxTokens || 30720
      };

      console.log('üíæ Guardando configuraci√≥n en la base de datos...');
      const saveResponse = await fetch(this.getApiConfigUrl(), {
        method: 'PUT',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(configToSave),
      });

      if (!saveResponse.ok) {
        const errorData = await saveResponse.json().catch(() => ({}));
        console.error('‚ùå Error al guardar la configuraci√≥n:', errorData);
        throw new Error('Error al guardar la configuraci√≥n en la base de datos');
      }

      // Actualizar la configuraci√≥n local con los datos guardados
      const savedConfig = await saveResponse.json();
      this.config = savedConfig;
      console.log('‚úÖ Configuraci√≥n guardada en la base de datos:', {
        provider: savedConfig.provider,
        model: savedConfig.model
      });

      // Reinicializar el servicio para asegurar que todo est√° sincronizado
      this.isInitialized = false;
      await this.initialize();

      console.log(`‚úÖ API key de ${provider} configurada y guardada correctamente`);
    } catch (error) {
      console.error(`‚ùå Error al configurar la API key de ${provider}:`, error);
      throw error;
    }
  }

  static async verificarEstadoAPIKey(): Promise<void> {
    console.log('üîç Verificando estado de la API key...');

    if (!this.config || !this.config.apiKey) {
      console.error('‚ùå No hay API key configurada');
      throw new Error('No hay API key configurada');
    }

    if (!this.config.provider) {
      console.error('‚ùå No hay proveedor configurado');
      throw new Error('No hay proveedor configurado');
    }

    try {
      console.log(`üì° Intentando verificar API key para ${this.config.provider}...`);
      
      const response = await fetch('/api/ai-config/verify', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          provider: this.config.provider,
          apiKey: this.config.apiKey
        }),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({}));
        console.error('‚ùå Error al verificar API key:', errorData);
        throw new Error(errorData.message || 'API key inv√°lida');
      }

      const data = await response.json();
      console.log('‚úÖ API key v√°lida:', data);

    } catch (error) {
      console.error('‚ùå Error al verificar API key:', error);
      throw error;
    }
  }

  static getGoogleApiKey(): string | null {
    return this.config?.apiKey || null;
  }

  static async setApiKey(apiKey: string): Promise<void> {
    try {
      const currentConfig = await this.getConfig();
      
      // Actualizar la API key en la base de datos
      const updatedConfig = await PrismaService.updateAIConfig(
        currentConfig.id,
        {
          ...currentConfig,
          apiKey
        }
      );

      if (!updatedConfig) {
        throw new Error('No se pudo actualizar la configuraci√≥n');
      }

      // Actualizar la configuraci√≥n en memoria asegurando que todos los campos requeridos est√©n presentes
      this.config = {
        id: updatedConfig.id || crypto.randomUUID(),
        provider: updatedConfig.provider || 'google',
        model: updatedConfig.model || 'gemini-pro',
        apiKey: updatedConfig.apiKey || null,
        temperature: updatedConfig.temperature || null,
        maxTokens: updatedConfig.maxTokens || null,
        systemPrompt: updatedConfig.systemPrompt || null,
        // Convertir expl√≠citamente los objetos de configuraci√≥n
        textProcessing: updatedConfig.textProcessing ? {
          tokenLimit: updatedConfig.textProcessing.tokenLimit || 8000,
          minLength: updatedConfig.textProcessing.minLength || 100,
          maxLength: updatedConfig.textProcessing.maxLength || 1000,
          language: updatedConfig.textProcessing.language || 'es',
          chunkSize: updatedConfig.textProcessing.chunkSize || 6000,
          processBySection: updatedConfig.textProcessing.processBySection || true,
          maintainArticleOrder: updatedConfig.textProcessing.maintainArticleOrder || true
        } : undefined,
        format: updatedConfig.format ? {
          includeMnemonicRules: updatedConfig.format.includeMnemonicRules || false,
          includePracticalCases: updatedConfig.format.includePracticalCases || false,
          includeCrossReferences: updatedConfig.format.includeCrossReferences || false
        } : undefined,
        feedback: updatedConfig.feedback ? {
          detailLevel: (updatedConfig.feedback.detailLevel as 'basic' | 'detailed') || 'detailed',
          includeNormativeReferences: updatedConfig.feedback.includeNormativeReferences || false,
          includeTopicConnections: updatedConfig.feedback.includeTopicConnections || false
        } : undefined,
        distribution: updatedConfig.distribution,
        createdAt: updatedConfig.createdAt || new Date(),
        updatedAt: updatedConfig.updatedAt || new Date()
      };
      
      // Reinicializar el servicio con la nueva API key
      this.isInitialized = false;
      await this.initialize();
    } catch (error) {
      console.error('Error al configurar la API key:', error);
      throw new Error('No se pudo guardar la API key');
    }
  }

  static async testApiKey(): Promise<void> {
    try {
      const selectedModel = this.getSelectedModel();
      if (!selectedModel) {
        throw new Error('No se ha seleccionado ning√∫n modelo');
      }

      if (!this.config?.apiKey) {
        throw new Error('No hay API key configurada');
      }

      let testEndpoint = '';
      let testBody = {};

      switch (selectedModel.provider) {
        case 'google':
          testEndpoint = `https://generativelanguage.googleapis.com/v1/models?key=${this.config.apiKey}`;
          break;
        case 'openai':
          testEndpoint = 'https://api.openai.com/v1/models';
          testBody = {
            headers: {
              'Authorization': `Bearer ${this.config.apiKey}`
            }
          };
          break;
        default:
          throw new Error(`Proveedor no soportado: ${selectedModel.provider}`);
      }

      const response = await fetch(testEndpoint, testBody);
      
      if (!response.ok) {
        const error = await response.json();
        throw new Error(`Error al validar la API key: ${error.error?.message || 'Error desconocido'}`);
      }

      console.log('‚úÖ API key v√°lida');
    } catch (error) {
      console.error('‚ùå Error al validar la API key:', error);
      throw error;
    }
  }

  static async verificarTodasLasAPIKeys(): Promise<Record<string, boolean>> {
    const resultados: Record<string, boolean> = {
      google: false,
      openai: false,
      anthropic: false
    };

    console.log('üîç Verificando todas las API keys...');

    // Verificar Google (Gemini)
    try {
      const geminiKey = process.env.NEXT_PUBLIC_GEMINI_API_KEY;
      if (geminiKey) {
        const response = await fetch(
          `https://generativelanguage.googleapis.com/v1/models?key=${geminiKey}`
        );
        resultados.google = response.ok;
        console.log(`Google API Key: ${response.ok ? '‚úÖ V√°lida' : '‚ùå Inv√°lida'}`);
      } else {
        console.log('‚ùå Google API Key no configurada');
      }
    } catch (error) {
      console.error('Error al verificar Google API Key:', error);
    }

    // Verificar OpenAI
    try {
      const openaiKey = process.env.NEXT_PUBLIC_GPT_API_KEY;
      if (openaiKey) {
        const response = await fetch('https://api.openai.com/v1/models', {
          headers: {
            'Authorization': `Bearer ${openaiKey}`
          }
        });
        resultados.openai = response.ok;
        console.log(`OpenAI API Key: ${response.ok ? '‚úÖ V√°lida' : '‚ùå Inv√°lida'}`);
      } else {
        console.log('‚ùå OpenAI API Key no configurada');
      }
    } catch (error) {
      console.error('Error al verificar OpenAI API Key:', error);
    }

    // Verificar Anthropic
    try {
      const anthropicKey = process.env.NEXT_PUBLIC_ANTHROPIC_API_KEY;
      if (anthropicKey) {
        // En lugar de hacer una llamada directa a la API, verificamos que la key tenga el formato correcto
        const isValidFormat = anthropicKey.startsWith('sk-ant-');
        resultados.anthropic = isValidFormat;
        console.log(`Anthropic API Key: ${isValidFormat ? '‚úÖ Formato v√°lido' : '‚ùå Formato inv√°lido'}`);
      } else {
        console.log('‚ùå Anthropic API Key no configurada');
      }
    } catch (error) {
      console.error('Error al verificar Anthropic API Key:', error);
    }

    return resultados;
  }

  // Configuraci√≥n de tipos de preguntas
  static async setQuestionTypes(types: any) {
    try {
      console.log('üéØ Configurando tipos de preguntas...');
      // Temporal: usar localStorage hasta resolver inconsistencia de tipos
      if (typeof window !== 'undefined') {
        localStorage.setItem('questionTypes', JSON.stringify(types));
      }
      console.log('‚úÖ Tipos de preguntas configurados correctamente');
      return { success: true, data: types };
    } catch (error) {
      console.error('‚ùå Error al configurar tipos de preguntas:', error);
      throw new Error('Error al configurar tipos de preguntas');
    }
  }

  // Configuraci√≥n de niveles de dificultad
  static async setDifficultyLevels(difficulties: any) {
    try {
      console.log('üéØ Configurando niveles de dificultad...');
      // Temporal: usar localStorage hasta resolver inconsistencia de tipos
      if (typeof window !== 'undefined') {
        localStorage.setItem('difficultyLevels', JSON.stringify(difficulties));
      }
      console.log('‚úÖ Niveles de dificultad configurados correctamente');
      return { success: true, data: difficulties };
    } catch (error) {
      console.error('‚ùå Error al configurar niveles de dificultad:', error);
      throw new Error('Error al configurar niveles de dificultad');
    }
  }

  // Configuraci√≥n de procesamiento de texto
  static async setTextProcessing(processingConfig: any) {
    try {
      console.log('üéØ Configurando procesamiento de texto...');
      // Usar AIConfig en lugar de QuestionConfig para textProcessing
      const currentConfig = await this.getConfig();
      const updatedConfig = {
        ...currentConfig,
        textProcessing: processingConfig
      };
      await this.setConfig(updatedConfig);
      console.log('‚úÖ Procesamiento de texto configurado correctamente');
      return updatedConfig;
    } catch (error) {
      console.error('‚ùå Error al configurar procesamiento de texto:', error);
      throw new Error('Error al configurar procesamiento de texto');
    }
  }

  // Configuraci√≥n de formato
  static async setFormatConfig(formatConfig: any) {
    try {
      console.log('üéØ Configurando formato de preguntas...');
      // Usar AIConfig en lugar de QuestionConfig para format
      const currentConfig = await this.getConfig();
      const updatedConfig = {
        ...currentConfig,
        format: formatConfig
      };
      await this.setConfig(updatedConfig);
      console.log('‚úÖ Formato de preguntas configurado correctamente');
      return updatedConfig;
    } catch (error) {
      console.error('‚ùå Error al configurar formato de preguntas:', error);
      throw new Error('Error al configurar formato de preguntas');
    }
  }

  // Configuraci√≥n de retroalimentaci√≥n
  static async setFeedbackConfig(feedbackConfig: any) {
    try {
      console.log('üéØ Configurando retroalimentaci√≥n...');
      // Usar AIConfig en lugar de QuestionConfig para feedback
      const currentConfig = await this.getConfig();
      const updatedConfig = {
        ...currentConfig,
        feedback: feedbackConfig
      };
      await this.setConfig(updatedConfig);
      console.log('‚úÖ Retroalimentaci√≥n configurada correctamente');
      return updatedConfig;
    } catch (error) {
      console.error('‚ùå Error al configurar retroalimentaci√≥n:', error);
      throw new Error('Error al configurar retroalimentaci√≥n');
    }
  }

  // Configuraci√≥n de distribuci√≥n
  static async setDistributionConfig(distributionConfig: any) {
    try {
      console.log('üéØ Configurando distribuci√≥n de preguntas...');
      // Usar AIConfig en lugar de QuestionConfig para distribution
      const currentConfig = await this.getConfig();
      const updatedConfig = {
        ...currentConfig,
        distribution: distributionConfig
      };
      await this.setConfig(updatedConfig);
      console.log('‚úÖ Distribuci√≥n de preguntas configurada correctamente');
      return updatedConfig;
    } catch (error) {
      console.error('‚ùå Error al configurar distribuci√≥n de preguntas:', error);
      throw new Error('Error al configurar distribuci√≥n de preguntas');
    }
  }

  // Configuraci√≥n de ratio te√≥rico/pr√°ctico
  static async setTheoryPracticeRatio(ratio: number) {
    try {
      console.log('üéØ Configurando ratio te√≥rico/pr√°ctico...');
      // Actualizar la configuraci√≥n dentro de distribution
      const currentConfig = await this.getConfig();
      const currentDistribution = currentConfig.distribution || {
        sectionDistribution: [],
        theoreticalPracticalRatio: 70,
        difficultyTypeDistribution: []
      };
      
      const updatedDistribution: DistributionConfig = {
        ...currentDistribution,
        theoreticalPracticalRatio: ratio
      };
      
      const updatedConfig = {
        ...currentConfig,
        distribution: updatedDistribution
      };
      await this.setConfig(updatedConfig);
      console.log('‚úÖ Ratio te√≥rico/pr√°ctico configurado correctamente');
      return updatedConfig;
    } catch (error) {
      console.error('‚ùå Error al configurar ratio te√≥rico/pr√°ctico:', error);
      throw new Error('Error al configurar ratio te√≥rico/pr√°ctico');
    }
  }

  // M√©todos para habilitar/deshabilitar caracter√≠sticas
  static async enableFeature(feature: string): Promise<void> {
    console.log(`üéØ Activando ${feature}...`);
    try {
      const currentFeatures = await this.getFeatures();
      const updatedFeatures = { ...currentFeatures, [feature]: true };
      
      const response = await fetch('/api/ai-features', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(updatedFeatures),
      });

      if (!response.ok) throw new Error(`Error al activar ${feature}`);
      console.log(`‚úÖ ${feature} activado y guardado en la API`);
    } catch (error) {
      console.error(`‚ùå Error al activar ${feature}:`, error);
      throw error;
    }
  }

  static async disableFeature(feature: string): Promise<void> {
    console.log(`üéØ Desactivando ${feature}...`);
    try {
      const currentFeatures = await this.getFeatures();
      const updatedFeatures = { ...currentFeatures, [feature]: false };
      
      const response = await fetch('/api/ai-features', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(updatedFeatures),
      });

      if (!response.ok) throw new Error(`Error al desactivar ${feature}`);
      console.log(`‚ùå ${feature} desactivado y guardado en la API`);
    } catch (error) {
      console.error(`‚ùå Error al desactivar ${feature}:`, error);
      throw error;
    }
  }

  // Hacer p√∫blico el m√©todo getFeatures
  static async getFeatures(): Promise<Record<string, boolean>> {
    const response = await fetch('/api/ai-features');
    if (!response.ok) throw new Error('Error al obtener caracter√≠sticas');
    return response.json();
  }

  // Agregar m√©todo updateFeature que espera el frontend
  static async updateFeature(feature: string, enabled: boolean): Promise<void> {
    try {
      console.log(`üéØ Actualizando ${feature}: ${enabled}`);
      const currentFeatures = await this.getFeatures();
      const updatedFeatures = { ...currentFeatures, [feature]: enabled };
      
      const response = await fetch('/api/ai-features', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(updatedFeatures),
      });

      if (!response.ok) throw new Error(`Error al actualizar ${feature}`);
      console.log(`‚úÖ ${feature} actualizado correctamente`);
    } catch (error) {
      console.error(`‚ùå Error al actualizar ${feature}:`, error);
      throw error;
    }
  }

  private static getDefaultModelForProvider(provider: AIProvider): AIModel {
    // Primero intentamos obtener el modelo de la configuraci√≥n actual
    if (this.config?.model) {
        const modelFromConfig = this.availableModels[provider]?.find(m => m.id === this.config?.model);
        if (modelFromConfig) {
            console.log('üìù Usando modelo de la configuraci√≥n:', modelFromConfig.name);
            return modelFromConfig;
        }
    }

    // Si no hay modelo en la configuraci√≥n o no es v√°lido, usamos el modelo Flash Thinking para Google
    if (provider === 'google') {
        const flashThinkingModel = this.availableModels[provider]?.find(m => m.id === 'gemini-2.0-flash-thinking');
        if (flashThinkingModel) {
            console.log('üìù Usando modelo Flash Thinking por defecto');
            return flashThinkingModel;
        }
    }

    // Si no encontramos el modelo espec√≠fico, usamos el primer modelo disponible del proveedor
    const models = this.availableModels[provider];
    if (!models || models.length === 0) {
        throw new Error(`No hay modelos disponibles para el proveedor ${provider}`);
    }

    console.log('üìù Usando primer modelo disponible:', models[0].name);
    return models[0];
  }

  static getAvailableModels(provider: AIProvider): AIModel[] {
    return this.availableModels[provider] || [];
  }

  public static async setModelAndProvider(provider: AIProvider, modelId: string): Promise<void> {
    console.log('üîÑ Cambiando modelo a:', modelId);
    try {
      // Buscar el modelo en el array global, filtrando por proveedor
      const model = availableModels.find(m => m.provider === provider && m.id === modelId);
      if (!model) {
        throw new Error(`Modelo ${modelId} no encontrado para el proveedor ${provider}`);
      }
      // Obtener la configuraci√≥n actual de la BD
      const response = await fetch(this.getApiConfigUrl());
      const currentConfig = await response.json();
      // Obtener la API key del environment para el nuevo proveedor
      const apiKey = this.getApiKeyFromEnv(provider);
      // Preparar nueva configuraci√≥n
      const newConfig = {
        provider: provider,
        model: modelId,
        apiKey: apiKey, // <-- Aqu√≠ se actualiza siempre
        temperature: currentConfig.temperature || model.config.temperature,
        maxTokens: currentConfig.maxTokens || model.config.maxTokens
      };
      // Guardar en la base de datos
      const saveResponse = await fetch(this.getApiConfigUrl(), {
        method: 'PUT',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(newConfig)
      });
      if (!saveResponse.ok) {
        throw new Error('Error al guardar la configuraci√≥n');
      }
      // Actualizar configuraci√≥n local
      this.config = await saveResponse.json();
      // Reinicializar el servicio
      this.isInitialized = false;
      await this.initialize();
      console.log('‚úÖ Modelo cambiado correctamente');
    } catch (error) {
      console.error('‚ùå Error al cambiar el modelo:', error);
      throw error;
    }
  }

  /**
   * Valida un texto usando la IA configurada y devuelve el feedback.
   * @param prompt Texto a validar (prompt completo)
   * @param model Modelo a usar (opcional)
   */
  public static async validateWithAI(prompt: string, model?: AIModel): Promise<string> {
    try {
      const selectedModel = model || this.getSelectedModel();
      if (!selectedModel) {
        throw new Error('No se ha seleccionado ning√∫n modelo de IA');
      }
      
      // Obtener API key
      const apiKey = this.getApiKeyFromEnv(selectedModel.provider);
      if (!apiKey) {
        throw new Error(`No se encontr√≥ API key para el proveedor ${selectedModel.provider}`);
      }
      
      return await this.callAIService(selectedModel, prompt, apiKey);
    } catch (error) {
      console.error('Error en validateWithAI:', error);
      throw error;
    }
  }

  /**
   * Obtiene la API key para un proveedor espec√≠fico
   * @param provider El proveedor para el que se desea obtener la API key
   * @returns La API key para el proveedor o null si no existe
   */
  static async getApiKeyForProvider(provider: AIProvider): Promise<string | null> {
    try {
      console.log(`üîë AIService.getApiKeyForProvider: Buscando API key para ${provider}`);
      
      // Verificar si ya tenemos la API key en la configuraci√≥n actual
      if (this.config && this.config.provider === provider && this.config.apiKey) {
        console.log(`‚úÖ Usando API key actual para ${provider}`);
        return this.config.apiKey;
      }
      
      // Intentar obtener la API key desde la API
      try {
        const response = await fetch(`/api/ai-provider-key?provider=${provider}`);
        console.log(`üìä Respuesta de la API: ${response.status}`);
        
        if (response.ok) {
          const data = await response.json();
          if (data.apiKey) {
            console.log(`‚úÖ API key recuperada desde la API para ${provider}`);
            return data.apiKey;
          }
        }
      } catch (error) {
        console.error(`‚ùå Error al obtener API key desde la API: ${error}`);
      }
      
      // Si no se encuentra en la API, usar la variable de entorno
      const envApiKey = this.getApiKeyFromEnv(provider);
      if (envApiKey) {
        console.log(`‚úÖ Usando API key de variables de entorno para ${provider}`);
        return envApiKey;
      }
      
      console.warn(`‚ö†Ô∏è No se encontr√≥ API key para ${provider}`);
      return null;
    } catch (error) {
      console.error('‚ùå Error en getApiKeyForProvider:', error);
      return null;
    }
  }

  async syncQuestionConfig(): Promise<void> {
    console.log('üîÑ AIService.syncQuestionConfig: Iniciando sincronizaci√≥n con la configuraci√≥n de preguntas...');
    try {
      const response = await fetch('/api/ai-config/sync-question-config', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
      });

      if (!response.ok) {
        throw new Error(`Error al sincronizar configuraci√≥n: ${response.status}`);
      }

      console.log('‚úÖ AIService.syncQuestionConfig: Sincronizaci√≥n completada con √©xito');
    } catch (error) {
      console.error('‚ùå AIService.syncQuestionConfig: Error durante sincronizaci√≥n:', error);
      throw error;
    }
  }
}

// A√±adir despu√©s de las importaciones y tipos
export function getOptionLengthInstruction(optionLength?: OptionLengthType): string {
  switch (optionLength) {
    case 'muy_corta':
      return `OBLIGATORIO: Cada opci√≥n de respuesta debe ser muy corta (m√°ximo 1-3 palabras, estilo test oficial). Si alguna opci√≥n supera ese l√≠mite, vuelve a acortarla.\nIMPORTANTE: Observa el ejemplo de pregunta con opciones muy cortas incluido en el prompt. Si no puedes cumplirlo, genera la opci√≥n m√°s breve posible.`;
    case 'media':
      return 'OBLIGATORIO: Cada opci√≥n de respuesta debe tener una longitud media (3-7 palabras, m√°ximo 15 palabras).';
    case 'larga':
      return 'OBLIGATORIO: Cada opci√≥n de respuesta debe ser extensa y detallada (1-2 frases, hasta 25 palabras).';
    case 'aleatoria':
      return 'Las opciones de respuesta pueden variar en longitud (corta, media o larga) de forma aleatoria.';
    case 'telegram':
      return `OBLIGATORIO TELEGRAM: L√çMITES ESTRICTOS DE CARACTERES:
- Pregunta (enunciado): M√ÅXIMO 300 caracteres (sin truncamiento)
- Cada opci√≥n de respuesta: M√ÅXIMO 100 caracteres (truncamiento autom√°tico si supera el l√≠mite)
- Explicaci√≥n (retroalimentaci√≥n): M√ÅXIMO 200 caracteres (con truncamiento permitido)

IMPORTANTE: Procura mantener las opciones dentro de 100 caracteres para evitar truncado autom√°tico. NO uses frases largas, NO uses explicaciones extensas en las opciones. Prioriza la claridad y brevedad extrema.`;
    default:
      return '';
  }
}

export const getAvailableModelsArray = (): AIModel[] => {
  return availableModels;
};

